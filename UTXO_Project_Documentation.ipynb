{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " UTXO Project Documentation.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "UnusVCr0mCYE",
        "abYnOH9vlKGG"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/williamzhao01123/crypto-lab/blob/master/UTXO_Project_Documentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-5lkqKHo3Op"
      },
      "source": [
        "#Create Joint_All in Bigquery with SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPcFjjrqkHUB"
      },
      "source": [
        "## Step 1: Query data from the input and output public data sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13l0Fxg0lGlA"
      },
      "source": [
        "### inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tng_aKxekw8B"
      },
      "source": [
        "In Bigquery, first choose destination table as"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpJ_2meVqbVK"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        "crypto-291811.UTXO.inputs\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMcNa6gTk3BT"
      },
      "source": [
        "Then put the SQL code in Bigquery Query BOX. Only the variables of interest are selected in this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciWR1wrlqTud"
      },
      "source": [
        "\r\n",
        "```\r\n",
        "SELECT \r\n",
        "  spent_transaction_hash,\r\n",
        "  spent_output_index,\t\r\n",
        "  block_timestamp AS spent_block_timestamp,   \r\n",
        "FROM `bigquery-public-data.crypto_bitcoin.inputs`\r\n",
        "```\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj7lOaLBlchA"
      },
      "source": [
        "###outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCNWtOkHlMA-"
      },
      "source": [
        "In Bigquery, first choose destination table as"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlMG5G6zqOM1"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        "crypto-291811.UTXO.outputs\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJoWxhdOlV6I"
      },
      "source": [
        "Then put the SQL code in Bigquery Query BOX. Only the variables of interest are selected in this step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X92qDrivqEvd"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        "SELECT \r\n",
        "  transaction_hash,\r\n",
        "  block_timestamp,\r\n",
        "  index,\t\r\n",
        "  value\r\n",
        "FROM `bigquery-public-data.crypto_bitcoin.outputs`\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIx5DFlGlhbd"
      },
      "source": [
        "## Step 2: Joint the two tables by transaction hash and index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrdlaqgVlvt0"
      },
      "source": [
        "Choose destination table as"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jx239ekr9R2"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        "crypto-291811.UTXO.joint\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqtO98s9lzGa"
      },
      "source": [
        "Put the SQL in Bigquery Query BOX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px7HNfLgsgzG"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        "SELECT\r\n",
        "  outputs.value,  \r\n",
        "  outputs.transaction_hash,\r\n",
        "  inputs.spent_transaction_hash,\r\n",
        "  outputs.index,\r\n",
        "  inputs.spent_output_index,\r\n",
        "  outputs.block_timestamp,\r\n",
        "  inputs.spent_block_timestamp,\r\n",
        "FROM \r\n",
        "  `crypto-291811.UTXO.outputs` AS outputs\r\n",
        "LEFT JOIN \r\n",
        "  `crypto-291811.UTXO.inputs` AS inputs\r\n",
        "ON outputs.transaction_hash=inputs.spent_transaction_hash  \r\n",
        "AND outputs.index = inputs.spent_output_index\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e19mIIT7l5na"
      },
      "source": [
        "For the above steps, you may also work in Google Colab. Reference the code here: https://github.com/sunshineluyao/UTXO/blob/main/UTXO_Age_Distribution_Data_queried_from_public_data_set.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnusVCr0mCYE"
      },
      "source": [
        "## Step 3: Format Data and Keep only variables of interest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpp8Tr_0nJAB"
      },
      "source": [
        "First choose destination table as"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpjhBT1J9Uia"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        "crypto-291811.UTXO.joint_all\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ku5-CLmOnKAd"
      },
      "source": [
        "Then put the SQL code in Bigquery Query BOX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osFRATWt9dYK"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        "SELECT \r\n",
        "  value,\r\n",
        "  FORMAT_TIMESTAMP(\"%Y-%m-%d\", block_timestamp) AS block_date,\r\n",
        "  FORMAT_TIMESTAMP(\"%Y-%m-%d\", spent_block_timestamp) AS spent_block_date,\r\n",
        "FROM\r\n",
        "  `crypto-291811.UTXO.joint`\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abYnOH9vlKGG"
      },
      "source": [
        "## Step 4: Partition and Cluster the Table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LML6GH9Y9lnK"
      },
      "source": [
        "After creating a data file that includes the information of value, block_date and spent_block_date, we would like to partition the table based on dates. This step will significantly save the cost of query in later data analysis work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wc9k87N-CDW"
      },
      "source": [
        "### Partitioning the data by born date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ6gX1g7-OuJ"
      },
      "source": [
        "A new table is created in bigquery by running the following SQL code in Query Box. This step costs 45GB query. As at most 4000 partitions are allowed in Bigquery, only data after 2012 is partitioned. Data before 2012 is small enough to be processed locally and seperately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur9Rg4rE-Meu"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        "CREATE TABLE\r\n",
        "  `crypto-291811.UTXO.joint_all_partitionedbyborn12`\r\n",
        "PARTITION BY\r\n",
        "  DATE(block_timestamp) AS\r\n",
        "SELECT\r\n",
        "  *\r\n",
        "WHERE\r\n",
        "  block_timestamp > TIMESTAMP('2012-01-01 00:00:00+00')\r\n",
        "FROM\r\n",
        "  `crypto-291811.UTXO.joint_all`\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOn6ku6T-zNs"
      },
      "source": [
        "### Partitioning the data by death date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EToeDdoy-5MV"
      },
      "source": [
        "Similarly, a second partitioned table is created with spent_block_date as the partition column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WgN-wxv_IiL"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        "CREATE TABLE\r\n",
        "  `crypto-291811.UTXO.joint_all_partitionedbydeath12`\r\n",
        "PARTITION BY\r\n",
        "  DATE(spent_block_timestamp) AS\r\n",
        "SELECT\r\n",
        "  *\r\n",
        "WHERE\r\n",
        "  spent_block_timestamp > TIMESTAMP('2012-01-01 00:00:00+00')\r\n",
        "FROM\r\n",
        "  `crypto-291811.UTXO.joint_all`\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qv7SWhK_OLh"
      },
      "source": [
        "An extra step that can be taken here is to further cluster the data by block_date. Clustering is another technique that could help save query cost in Bigquery. There are no limits on the number of clusters to be created, but the benefits clustering brings is much smaller than partitioning. For example, the SQL below creates a table partitioned by spent_block_date but clustered by block_date."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEzdH6bm_zOG"
      },
      "source": [
        "\r\n",
        "\r\n",
        "```\r\n",
        "CREATE TABLE\r\n",
        "  `crypto-291811.UTXO.joint_all_pdeath_cborn`\r\n",
        "PARTITION BY\r\n",
        "  DATE(spent_block_timestamp) \r\n",
        "  CLUSTER BY DATE(block_timestamp) AS\r\n",
        "SELECT\r\n",
        "  *\r\n",
        "WHERE\r\n",
        "  spent_block_timestamp > TIMESTAMP('2012-01-01 00:00:00+00')\r\n",
        "FROM\r\n",
        "  `crypto-291811.UTXO.joint_all`\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUwL6UbGAFGi"
      },
      "source": [
        "# Processing the Data in Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYl1eSOCAOdC"
      },
      "source": [
        "With the data file joint_all ready in Bigquery, we process the UTXO data in kaggle to further refine the information. The kaggle project can be found at [UTXO Project](https://www.kaggle.com/williamzhao123/utxo-project-william-zhao)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcwQXN4vB-88"
      },
      "source": [
        "##Step 1: Import Data into Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjbCSIXPFfcg"
      },
      "source": [
        "###From Big Query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tKn3PGkFrOL"
      },
      "source": [
        "Kaggle can read data by running a query in Bigquery if the account is authorized. An example program is shown below, though this notebook is not authorized to query data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWaE7dcHG2Ur"
      },
      "source": [
        "PROJECT_ID = 'crypto-291811'\r\n",
        "\r\n",
        "from datetime import date, timedelta\r\n",
        "from google.cloud import bigquery\r\n",
        "client = bigquery.Client(project=PROJECT_ID, location='US')\r\n",
        "dataset_ref = client.dataset('UTXO', project=PROJECT_ID)\r\n",
        "dataset = client.get_dataset(dataset_ref)\r\n",
        "tables = list(client.list_tables(dataset))\r\n",
        "# Print names of all tables in the dataset\r\n",
        "for table in tables:  \r\n",
        "    print(table.table_id)\r\n",
        "\r\n",
        "start_date = date(2013,1,1)\r\n",
        "end_date = start_date+timedelta(days=1)\r\n",
        "query1 = \"\"\"\r\n",
        "      SELECT \r\n",
        "        *\r\n",
        "      FROM \r\n",
        "        `crypto-291811.UTXO.joint_all_partitionedbyborn12`\r\n",
        "      WHERE\r\n",
        "        block_timestamp >= TIMESTAMP('\"\"\" + str(start_date) + \"\"\" 00:00:00+00')\r\n",
        "      AND \r\n",
        "        block_timestamp < TIMESTAMP('\"\"\" + str(end_date) + \"\"\" 00:00:00+00')\"\"\"\r\n",
        "query_job1 = client.query(query1)\r\n",
        "# Make an API request  to run the query and return a pandas DataFrame\r\n",
        "data = query_job1.to_dataframe()\r\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0myQrblXHygj"
      },
      "source": [
        "We feed the UTXO data after 2012 day by day to kaggle with the above approach. As the data is partitioned by block_date or spent_block_date, filtering data either block_date or spent_block_date is very efficient in Bigquery."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6Boj9ANIRIg"
      },
      "source": [
        "###From Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gs4fG3xIXDH"
      },
      "source": [
        "Data before 2012, on the other hand, is imported to kaggle from Google Drive. When we partitioned the data, only data after 2012 is kept in Bigquery in order to limit number of partitions to 4000. As the data before 2011 is small enough to use as a whole, we import the data file from Google Drive directly with the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjUI-MFzI2rq"
      },
      "source": [
        "!conda install -y gdown\r\n",
        "import gdown\r\n",
        "url = 'https://drive.google.com/uc?id=1-83ycaEkWjV0TsD7875RL9uRdXSIh9_D'\r\n",
        "output = 'joint_2011.csv'\r\n",
        "gdown.download(url, output, quiet=False)\r\n",
        "data = pd.read_csv('joint_2011.csv')\r\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTaMhLsyJMEk"
      },
      "source": [
        "##Step 2: Processing Daily Data in Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bz4fqGPJSJI"
      },
      "source": [
        "While the data before 2012 and after 2012 are imported seperately into kaggle, data are analyzed in the same way after partitioned on a daily basis. We carry out three tasks that reveal the born and death, life expectancy, and longetivity distribution of UTXO in Kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I19ilPTRMSJ7"
      },
      "source": [
        "###Task 1: UTXO born and death"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7tIJOfjMURx"
      },
      "source": [
        "In Task 1, we compute the number of UTXOs created and spent on each day. To see how many UTXOs were born in a day, we query the data from Bigquery that only keeps the UTXOs with block_timestamp within that day. We then sum the UTXOs in each transaction block to get the total UTXOs born in the day. Similarly, we query Bigquery again to keep the UTXOs with spent_block_timestamp within that day to get the total number of UTXOs spent on that date.\r\n",
        "\r\n",
        "With these, it is possible to further compute the net new UTXOs in each day by minusing the number of UTXOs dead from the number of UTXOs born. The cumulative sum of net new UTXOs is the total number of UTXOs existing at each day.\r\n",
        "\r\n",
        "> $$Net\\_new[day = i] = Born[day = i] - Death[day = i] $$\r\n",
        "> $$Cum[day = i] = \\sum_{i \\leq date} Net\\_new[day = i]$$\r\n",
        "\r\n",
        "In the kaggle notebook, Task1_born(data) and Task1_dead(data) are two functions defined to carry out this part of analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imKWbcPsNGTp"
      },
      "source": [
        "###Task 2: UTXO weighted average life expectancy (WALE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_caoGzsLPtw6"
      },
      "source": [
        "Weighted average life expectancy refers to the average length of living (from created to spent) of UTXOs that are spent before a given date, weighted by the value of UTXO. This shows how fast a UTXO is spent on average. To measure this, we query the data with spent_block_timestamp within each day to get the average life expectancy of UTXOs spent **on** that date.\r\n",
        "\r\n",
        "> $$Life\\_length = spent\\_block\\_timestamp - block\\_timestamp$$\r\n",
        "> $$WALE(day = i) = \\sum_{day=i} (UTXO * Life\\_length)/Death[day=i]$$\r\n",
        "\r\n",
        "After calculating the WALE on each date, it is possible to calculate the WALE for the UTXOs spent **before** a given date:\r\n",
        "\r\n",
        "> $$WALE(day\\leq today)=\\frac{\\sum_{i \\leq today}WALE(day=i)*Death[day=i]}{\\sum_{i\\leq today}Death[day=today]} $$\r\n",
        "\r\n",
        "In the kaggle notebook, Task2(data) is defined to carry out this part of analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJI5_9KJXfyt"
      },
      "source": [
        "###Task 3: UTXO Life Length Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyAm0qbYXskO"
      },
      "source": [
        "The life length of a UTXO is the difference between its spent_block_timestamp and its block_timestamp, i.e. the gap between the time created and the time spent. We would like to see the distribution of UTXOs' life length up to some date. To do this, we first compute the life length of each UTXO spent on a specific date and categorize them into the following categories: <1d, 1d-1m, 1m-1q, 1q-6m, 6m-1y, 1y-2y, 2y-3y, 3y-4y, 4y-5y, >5y. A categorical map is applied on life length to get categories. The total value of UTXOs in a category that are spent before some date can be computed by spent date:\r\n",
        "\r\n",
        "> $$C_k[day \\leq today] = \\sum_{i\\leq today} C_k[day = i] $$\r\n",
        "\r\n",
        "In the kaggle notebook, Task3(data) is defined to carry out this part of analysis. cal(x) is the categorical map we use. Task3_process(result) is used to transform the analysis result to a form easy to record and visualize by expanding the distribution data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qQ0_9HI1e4c"
      },
      "source": [
        "#Visualizing Data in Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrmdO6onOcN-"
      },
      "source": [
        "With the raw data processed and transformed to time series easy to visualize, several visualizations are created with the package plotly. The visualizations are made in the following Colab Notebook: [UTXO Visualizations](https://colab.research.google.com/drive/17B6YxJrVeI2qhTHAe5FaJxGqb92SR0jq?usp=sharing)"
      ]
    }
  ]
}